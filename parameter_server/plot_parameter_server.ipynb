{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Parameter Server\n",
    "\n",
    "The parameter server is a framework for distributed machine learning training.\n",
    "\n",
    "In the parameter server framework, a centralized server (or group of server nodes) maintains global shared arameters of a machine-learning model (e.g., a neural network) while the data and computation of calculating updates (i.e., gradient descent updates) are distributed over worker nodes.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/master/_images/param_actor1.png\" align=\"center\">\n",
    "\n",
    "Parameter servers are a core part of many machine learning applications. This document walks through how to implement simple synchronous and asynchronous parameter servers using Ray actors.\n",
    "\n",
    "Let's first define some helper functions and import some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "from lithops import multiprocessing as mp\n",
    "from lithops.multiprocessing.managers import SyncManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader():\n",
    "    \"\"\"Safely downloads data. Returns training/validation set dataloader.\"\"\"\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"~/data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=mnist_transforms),\n",
    "        batch_size=128,\n",
    "        shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n",
    "        batch_size=128,\n",
    "        shuffle=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Evaluates the accuracy of the model on a validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # This is only set to finish evaluation faster.\n",
    "            if batch_idx * len(data) > 1024:\n",
    "                break\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100. * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a small neural network to use in training. We provide some helper functions for obtaining data, including getter/setter methods for gradients and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"Small ConvNet for MNIST.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
    "        self.fc = nn.Linear(192, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, 192)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {k: v.cpu() for k, v in self.state_dict().items()}\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.load_state_dict(weights)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        grads = []\n",
    "        for p in self.parameters():\n",
    "            grad = None if p.grad is None else p.grad.data.cpu().numpy()\n",
    "            grads.append(grad)\n",
    "        return grads\n",
    "\n",
    "    def set_gradients(self, gradients):\n",
    "        for g, p in zip(gradients, self.parameters()):\n",
    "            if g is not None:\n",
    "                p.grad = torch.from_numpy(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Server using Manager\n",
    "\n",
    "We will define a Manager instance that will hold a copy of the model and contain the logic for, during training, receive gradients and apply them to its model and to send the updated model back to the workers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterServer:\n",
    "    def __init__(self, lr):\n",
    "        self.model = ConvNet()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def apply_gradients(self, *gradients):\n",
    "        summed_gradients = [\n",
    "            np.stack(gradient_zip).sum(axis=0)\n",
    "            for gradient_zip in zip(*gradients)\n",
    "        ]\n",
    "        self.optimizer.zero_grad()\n",
    "        self.model.set_gradients(summed_gradients)\n",
    "        self.optimizer.step()\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SyncManager.register('ParameterServer', ParameterServer)\n",
    "\n",
    "man = SyncManager()\n",
    "ps = man.ParameterServer(1e-2)\n",
    "ps_lock = man.Lock()\n",
    "\n",
    "man.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The worker will also hold a copy of the model. During training. it will continuously evaluate data and send gradients to the parameter server. The worker will synchronize its model with the Parameter Server model weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(parameter_server, lock):\n",
    "    model = ConvNet()\n",
    "    data_iterator = iter(get_data_loader()[0])\n",
    "    with lock:\n",
    "        weights = parameter_server.get_weights()\n",
    "    model.set_weights(weights)\n",
    "    try:\n",
    "        data, target = next(data_iterator)\n",
    "    except StopIteration:  # When the epoch ends, start a new epoch.\n",
    "        data_iterator = iter(get_data_loader()[0])\n",
    "        data, target = next(data_iterator)\n",
    "    model.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    gradients = model.get_gradients()\n",
    "    with lock:\n",
    "        parameter_server.apply_gradients(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use a Pool to spawn workers that interact with the manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 5\n",
    "num_workers = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running synchronous parameter server training.\")\n",
    "\n",
    "pool = mp.Pool()\n",
    "model = ConvNet()\n",
    "test_loader = get_data_loader()[1]\n",
    "\n",
    "for i in range(iterations):\n",
    "    pool.starmap(compute_gradients, [(ps, ps_lock)] * num_workers)\n",
    "    with ps_lock:\n",
    "        current_weights = ps.get_weights()\n",
    "\n",
    "    # Evaluate the current model.\n",
    "    model.set_weights(current_weights)\n",
    "    accuracy = evaluate(model, test_loader)\n",
    "    print(\"Iter {}: \\taccuracy is {:.1f}\".format(i, accuracy))\n",
    "\n",
    "print(\"Final accuracy is {:.1f}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Server using Processes and Queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_server(lr, server_queue, worker_queues, iterations):\n",
    "    model = ConvNet()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    test_loader = get_data_loader()[1]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print(f'Iteration {i}', flush=True)\n",
    "        weights = model.get_weights()\n",
    "        \n",
    "        for q in worker_queues:\n",
    "            print('put', flush=True)\n",
    "            q.put(weights)\n",
    "        \n",
    "        gradients = []\n",
    "        for _ in range(len(worker_queues)):\n",
    "            print('Received gradient', flush=True)\n",
    "            gradient = server_queue.get(timeout=15)\n",
    "            gradients.append(gradient)\n",
    "        \n",
    "        summed_gradients = [\n",
    "            np.stack(gradient_zip).sum(axis=0)\n",
    "            for gradient_zip in zip(*gradients)\n",
    "        ]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.set_gradients(summed_gradients)\n",
    "        optimizer.step()\n",
    "        accuracy = evaluate(model, test_loader)\n",
    "        print(f'Accuracy is {accuracy}', flush=True)\n",
    "    \n",
    "    for q in worker_queues:\n",
    "        print('terminate', flush=True)\n",
    "        q.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(queue, param_server_queue):\n",
    "    model = ConvNet()\n",
    "    data_iterator = iter(get_data_loader()[0])\n",
    "    \n",
    "    while True:\n",
    "        weights = queue.get(timeout=35)\n",
    "\n",
    "        if weights is None:\n",
    "            break\n",
    "        \n",
    "        model.set_weights(weights)\n",
    "        try:\n",
    "            data, target = next(data_iterator)\n",
    "        except StopIteration:  # When the epoch ends, start a new epoch.\n",
    "            data_iterator = iter(get_data_loader()[0])\n",
    "            data, target = next(data_iterator)\n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        gradients = model.get_gradients()\n",
    "        \n",
    "        param_server_queue.put(gradients)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 25\n",
    "iterations = 5\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.config.set_parameter(mp.config.STREAM_STDOUT, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_q = mp.Queue()\n",
    "worker_queues = [mp.Queue() for _ in range(num_workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_server = mp.Process(target=parameter_server, args=(lr, server_q, worker_queues, iterations))\n",
    "param_server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with mp.Pool() as p:\n",
    "    p.starmap(worker, [(q, server_q) for q in worker_queues])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_server.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lithops",
   "language": "python",
   "name": "lithops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
